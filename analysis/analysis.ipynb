{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b25186-01ae-494c-9283-91d73fc7c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.random import Generator, PCG64\n",
    "rng = Generator(PCG64())\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4f6779-44b7-4b1a-8fdd-65de8a01859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b7fbb-70ba-4a31-a2ff-56318a039f6a",
   "metadata": {},
   "source": [
    "# Trial Parition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bfe24d-4bd1-4bc6-b568-2c60193e508e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants:  426\n"
     ]
    }
   ],
   "source": [
    "responses = pd.read_csv(\"https://data-visualization-benchmark.s3.us-west-2.amazonaws.com/vt-fusion/participant_responses.csv\")\n",
    "performance_df = responses.copy()\n",
    "print(\"Number of participants: \", len(responses[['participant_id']].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0053383a-ecbe-4a4d-9f00-a702159ef6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# responses.columns\n",
    "responses['image_file'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d09eb36-bedc-403c-b43f-b49fd3b89b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"More than 60% of residents have a Bachelor's degree or above.\",\n",
       " 'The majority of residents have a graduate degree.',\n",
       " 'There are fewer residents whose education level is Associate degree than those with a high school degree.',\n",
       " 'None of the above.',\n",
       " 'Which of the following is true about the education level of residents in city Z?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_df.iloc[-1][['mc1', 'mc2', 'mc3', 'mc4', 'question']].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5c7c6-462e-4a47-8fc9-1054851a356c",
   "metadata": {},
   "source": [
    "### Substitute task categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1406e4-e807-4e7a-a677-8d6b22420d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlat_format_map = {\n",
    "    'find_extremum': 'value identification', \n",
    "    'determine_Range': 'value identification', \n",
    "    'retrieve_value': 'value identification',\n",
    "    'make_comparisons': 'arithmetic computation',\n",
    "    'find_correlations_trends': 'statistical inference', \n",
    "    'characterize_distribution': 'statistical inference',\n",
    "    'find_anomolies': 'statistical inference', \n",
    "    'find_clusters': 'statistical inference', \n",
    "}\n",
    "\n",
    "calvi_format_map = {\n",
    "    'Make Comparisons': 'arithmetic computation',\n",
    "    'Find Correlations/Trends': 'statistical inference', \n",
    "    'Find Extremum': 'value identification',\n",
    "    'Retrieve Value': 'value identification', \n",
    "    'Make Predictions': 'statistical inference', \n",
    "    'Aggregate Values': 'statistical inference'\n",
    "}\n",
    "\n",
    "brbf_task_map = {\n",
    "    'max': \"value identification\", \n",
    "    'min': 'value identification', \n",
    "    'trend': 'statistical inference',\n",
    "    'trendComp': 'arithmetic computation', \n",
    "    'average': 'statistical inference', \n",
    "    'intersection': 'value identification',\n",
    "}\n",
    "\n",
    "wainer_question_map = {\n",
    "    \"How many months have more rain than the average month?\": \"statistical inference\",\n",
    "    \"Which season has more rain, the summer or the spring?\": \"arithmetic computation\",\n",
    "    \"Which season has the most rain?\": \"value identification\",\n",
    "    \"In San Francisco, as the weather gets warmer, there is generally more rain.\": \"statistical inference\",\n",
    "    \"In which season does each month have less rain than the month before?\": \"arithmetic computation\",\n",
    "    \"How many months have less than 40mm of rain?\": \"arithmetic computation\",\n",
    "    \"How much does it rain in March?\": \"value identification\",\n",
    "    \"Which month has 25 mm of rain?\": \"value identification\"\n",
    "}\n",
    "\n",
    "ggr_question_map = {\n",
    "    # from level 1\n",
    "    \"Approximately what percentage of people had Adeolitis in the year 2000?\": \"value identification\",\n",
    "    \"What percentage of patients recovered after chemotherapy?\": \"value identification\",\n",
    "    \"Of 100 patients with disease X, how many are women?\": \"value identification\",\n",
    "    \"Of all the people who die from cancer, approximately what percentage dies from lung cancer?\": \"value identification\",\n",
    "\n",
    "    # from level 2\n",
    "    \"Approximately what percentage of people who die from cancer die from colon cancer, breast cancer, and prostate cancer taken together?\": \"arithmetic computation\",\n",
    "    \"How many more men than women are there among 100 patients with disease X?\": \"arithmetic computation\",\n",
    "    \"What is the difference between the percentage of patients who recovered after a surgery and the percentage of patients who recovered after radiation therapy?\": \"arithmetic computation\",\n",
    "    \"When was the increase in the percentage of people with Adeolitis higher?\": \"arithmetic computation\",\n",
    "\n",
    "    # from level 3\n",
    "    \"What percentage of patients recovered after chemotherapy?\": \"value identification\",\n",
    "    \"According to your best guess, what will the percentage of people with Adeolitis be in the year 2010?\": \"statistical inference\",\n",
    "    \"Between 1980 and 1990, which disease had a higher increase in the percentage of people affected?\": \"statistical inference\",\n",
    "    \"Compared to the placebo, which treatment leads to a larger decrease in the percentage of patients who die?\": \"statistical inference\",\n",
    "    \"What is the percentage of cancer patients who die after chemotherapy?\": \"value identification\",\n",
    "    \"Which of the treatments contributes to a larger decrease in the percentage of sick patients?\": \"statistical inference\"\n",
    "}\n",
    "\n",
    "task_category_map = ggr_question_map | wainer_question_map | brbf_task_map | vlat_format_map | calvi_format_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa896750-1c07-4a44-96fc-f5e392a3cf30",
   "metadata": {},
   "source": [
    "### Substitute chart categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba88366e-9102-4836-857c-886f65695155",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_categories = {\n",
    "    'Dot Plot': 'Dot Plot',\n",
    "    'Line Chart': 'Line',\n",
    "    'Pie Chart': 'Pie',\n",
    "    'chart': 'Table',\n",
    "    'radial': 'Radial',\n",
    "    'line': 'Line',\n",
    "    'bar': 'Bar',\n",
    "    'Scatterplot': 'Scatter',\n",
    "    'Bar chart': 'Bar',\n",
    "    'Stacked area chart': 'Stacked Area',\n",
    "    '100% stacked bar chart': '100% Stacked Bar',\n",
    "    'Line chart': 'Line',\n",
    "    'Choropleth map': 'Map',\n",
    "    'Area chart': 'Area',\n",
    "    'Choropleth Map': 'Map',\n",
    "    'Pie chart': 'Pie',\n",
    "    'Stacked bar chart': 'Stacked Bar',\n",
    "    'bg-table': 'Table',\n",
    "    'bg': 'Bar',\n",
    "    'sp-table': 'Table',\n",
    "    'sp': 'Scatter',\n",
    "    'lg2': 'Line',\n",
    "    'lg1-table': 'Table',\n",
    "    'lg1': 'Line',\n",
    "    'lg2-table': 'Table',\n",
    "    'Bar Chart': 'Bar',\n",
    "    'Bubble Chart': 'Scatter',\n",
    "    'Area Chart': 'Area',\n",
    "    '100 % Stacked Bar Chart': '100% Stacked Bar',\n",
    "    'Stacked Area Chart': 'Stacked Area',\n",
    "    'Stacked Bar Chart': 'Stacked Bar',\n",
    "    'Treemap': 'Treemap',\n",
    "    'table': 'Table'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d07cc-cc5e-4d1b-93d8-0de3faa43ade",
   "metadata": {},
   "source": [
    "### Create item dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b117b66b-db18-401a-80ba-08824f3a4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_item_df():\n",
    "    tests = ['wainer', 'brbf', 'ggr-mc', 'vlat', 'calvi']\n",
    "    item_df = []\n",
    "    for test in tests:\n",
    "        idf = pd.read_csv(f'https://data-visualization-benchmark.s3.us-west-2.amazonaws.com/{test}/questions.csv')\n",
    "        idf['test_type'] = test\n",
    "        item_df.append(idf)\n",
    "\n",
    "    item_df = pd.concat(item_df)\n",
    "    item_df['question_image'] = item_df['question'] + \" + \" + item_df['image_file']\n",
    "    return item_df\n",
    "    \n",
    "def add_brbf_categories(r):\n",
    "    if (r['test_type'] == 'brbf'):\n",
    "        return '-'.join(r['image_file'].split(\"-\")[:-1])\n",
    "    return r['chart_type']\n",
    "\n",
    "\n",
    "item_df = create_item_df()\n",
    "item_df['chart_type_filled'] = item_df.apply(add_brbf_categories, axis=1)\n",
    "item_df['chart_type'] = item_df['chart_type_filled'].replace(chart_categories)\n",
    "# item_df = item_df[item_df['chart_type'] != 'Table']\n",
    "\n",
    "item_df['task_category'] = item_df['task_category'].apply(\n",
    "    lambda t : task_category_map[t] if t in task_category_map.keys() else t\n",
    ")\n",
    "item_df['task_category'] = item_df.apply(\n",
    "    lambda r : task_category_map[r['question']] if r['question'] in task_category_map.keys() else r['task_category'],\n",
    "    axis=1\n",
    ")\n",
    "item_df['task_category'] = item_df['task_category'].apply(lambda x : x.replace(\" \", \"-\")).to_numpy()\n",
    "item_df['chart_type'] = item_df['chart_type'].apply(lambda x : x.replace(\" \", \"-\")).to_numpy()\n",
    "\n",
    "item_df['item_id'] = item_df['question_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea27e159-0a5c-431a-919e-59012aa6db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(\n",
    "        data: pd.DataFrame,\n",
    "        measure,\n",
    "        id_col,\n",
    "        n_iterations=1000,\n",
    "        statistic=np.mean\n",
    "    ):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    items = list(data[id_col].unique())\n",
    "    n_size = len(items)\n",
    "    df = data.copy()\n",
    "\n",
    "    def bootstrap_iteration(data, chosen_items):\n",
    "        # filter_df = data[data[id_col].isin(chosen_items)] # Filter based on chosen questions\n",
    "        filter_df = pd.concat(\n",
    "            [data[data[id_col].isin([item])] for item in chosen_items]\n",
    "        )\n",
    "        \n",
    "        bs_mean = statistic(filter_df[measure]) \n",
    "        return (bs_mean, list(chosen_items))\n",
    "\n",
    "    qset_means = Parallel(n_jobs=-1)(\n",
    "        delayed(bootstrap_iteration)(\n",
    "            df.copy(),\n",
    "            rng.choice(items, n_size,  replace=True)\n",
    "        ) for _ in range(n_iterations)\n",
    "    )\n",
    "    \n",
    "    means = []\n",
    "    qs_used = []\n",
    "    means = [bs_mean for bs_mean, chosen_qs in qset_means]\n",
    " \n",
    "    # 95% confidence interval\n",
    "    lower = np.percentile(means, 2.5)\n",
    "    upper = np.percentile(means, 97.5)\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "def create_confidence_interval_df(\n",
    "    data: pd.DataFrame,\n",
    "    measure, \n",
    "    id_col,\n",
    "    condition_col,\n",
    "    statistic=np.mean\n",
    "):\n",
    "    \"\"\" create the dataframe for 95% bootstrapped confidence interval\n",
    "    data: dataset\n",
    "    measure: dependent variable\n",
    "    id_col: units to bootstrap along (e.g. participant_d or item_id)\n",
    "    condition_col: the different conditions of the experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    data_list = []\n",
    "\n",
    "    for condition in data[condition_col].unique():\n",
    "        condition_data = data[data[condition_col] == condition]\n",
    "\n",
    "        lower, upper = bootstrap_ci(condition_data, measure=measure, statistic=statistic, id_col=id_col)\n",
    "        mean = condition_data[measure].mean()\n",
    "        \n",
    "        data_list.append({\n",
    "            \"category\": condition,\n",
    "            \"mean\": mean,\n",
    "            \"ci_upper\": upper, \n",
    "            \"ci_lower\": lower,\n",
    "        })\n",
    "\n",
    "    ci_df = pd.DataFrame(data_list)\n",
    "\n",
    "    item_level_data = data.rename(columns={\n",
    "        condition_col: 'category', \n",
    "    }).groupby([id_col, 'category'])[measure].mean().reset_index()\n",
    "\n",
    "    return item_level_data, ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79bcef03-857e-4405-bfac-e75ea0e39072",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_color_map = {\n",
    "    'wainer': '#3a0ca3',\n",
    "    'ggr-mc': '#e26d5c',\n",
    "    'brbf': '#f4a261',\n",
    "    'vlat': '#247ba0',\n",
    "    'calvi': '#70c1b3'\n",
    "}\n",
    "\n",
    "\n",
    "def box_muller():\n",
    "    '''\n",
    "    apparently controls how much jitter but who knows :shrug:\n",
    "    '''\n",
    "    # jitter_level * math.sqrt(-2 * math.log(random.random())) #* math.cos(2 * math.pi * random.random())\n",
    "    return rng.normal() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ebbe2-24d9-4b3c-aef4-837cb2b08582",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b87521b-2e70-427d-8d08-f878d5aa83b5",
   "metadata": {},
   "source": [
    "## How does performance vary across items?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae5ddda-fbce-4b45-9df3-38ce02ac258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_amount_at_chance(r):\n",
    "    option_count = 0\n",
    "    for i in range(1, 11):\n",
    "        if not pd.isna(r[f'mc{i}']):\n",
    "            option_count += 1\n",
    "    return 1 / option_count\n",
    "\n",
    "item_df['chance_selection'] = item_df.apply(find_amount_at_chance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0adcca-25ec-43c7-a37f-1d0fe27d4f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading item-performance dataframes\n",
      "Regenerating item-performance dataframes\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Reloading item-performance dataframes\")\n",
    "    item_ci_df = pd.read_csv(\"./dataframes/item_ci_df.csv\")\n",
    "    item_participant_level_data = pd.read_csv(\"./dataframes/item_participant_level_data.csv\")\n",
    "    \n",
    "except:\n",
    "    print(\"Regenerating item-performance dataframes\")\n",
    "    item_participant_level_data, item_ci_df = create_confidence_interval_df(\n",
    "        performance_df, \n",
    "        measure='is_correct',\n",
    "        id_col='participant_id',\n",
    "        condition_col='question_image',\n",
    "    )\n",
    "    item_ci_df = pd.merge(item_ci_df, item_df[['question_image', 'test_type']].rename(columns={'question_image': 'category'}))\n",
    "    item_participant_level_data.to_csv(\"./dataframes/item_participant_level_data.csv\")\n",
    "    item_ci_df.to_csv(\"./dataframes/item_ci_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07316f8-e368-4c17-aed3-c3e25c20e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessment_color_map = {\n",
    "    'wainer': '#8947BF', # '#3a0ca3',\n",
    "    'ggr-mc': '#e26d5c',\n",
    "    'brbf': '#f4a261',\n",
    "    'vlat': '#247ba0',\n",
    "    'calvi': '#70c1b3'\n",
    "}\n",
    "\n",
    "item_ci_df_sorted = item_ci_df.sort_values(by='mean', ascending=False)\n",
    "item_domain_order = item_ci_df_sorted['category'].to_list()\n",
    "\n",
    "test_domain = list(assessment_color_map.keys())\n",
    "test_domain_color = list(assessment_color_map.values())\n",
    "\n",
    "def create_item_accuracy_chart(ci_df, domain_order):\n",
    "    \"\"\" Generate chart for point estimate and confidence intervals overlayed with item level scatteplots\n",
    "    \"\"\"\n",
    "    # Create dot+error bar plot\n",
    "    chart = alt.Chart(ci_df).mark_bar(opacity=0.7).encode(\n",
    "        x=alt.X('category:N', title='Category', scale=alt.Scale(domain=domain_order), axis=None),\n",
    "        y=alt.Y('mean:Q', title='Prop. Correct'),\n",
    "        color=alt.Color('test_type:N', scale=alt.Scale(domain=test_domain, range=test_domain_color), legend=None),\n",
    "    ) + alt.Chart(ci_df).mark_rule(strokeWidth=0.4, opacity=1).encode(\n",
    "        x=alt.X('category:N'),\n",
    "        y=alt.Y('ci_lower:Q'),\n",
    "        y2='ci_upper:Q',\n",
    "        color=alt.value(\"#717d7e\")\n",
    "        # color=alt.Color('test_type:N', scale=alt.Scale(domain=test_domain, range=test_domain_color), legend=None),\n",
    "    )\n",
    "    \n",
    "    return chart.properties(width=800, height=200)\n",
    "\n",
    "item_performance_reliability_plot = create_item_accuracy_chart(\n",
    "    item_ci_df, \n",
    "    domain_order=item_domain_order,\n",
    ")\n",
    "# item_performance_reliability_plot.save(\"./figures/item_performance_reliability.pdf\")\n",
    "item_performance_reliability_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215cc9a-3cad-4475-adbc-15c913544381",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ci_df = pd.merge(item_df[['item_id', 'chance_selection']], item_ci_df, left_on='item_id', right_on='category')\n",
    "item_ci_df['mean'] = item_ci_df['mean'] - item_ci_df['chance_selection']\n",
    "item_ci_df['ci_upper'] = item_ci_df['ci_upper'] - item_ci_df['chance_selection']\n",
    "item_ci_df['ci_lower'] = item_ci_df['ci_lower'] - item_ci_df['chance_selection']\n",
    "\n",
    "item_performance_reliability_below_chance_plot = create_item_accuracy_chart(\n",
    "    item_ci_df, \n",
    "    domain_order=item_domain_order,\n",
    ")\n",
    "item_performance_reliability_below_chance_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b9812-c086-4d93-b699-394e5d714f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_ci_df['err_diff'] =  item_ci_df['ci_upper'] - item_ci_df['ci_lower']\n",
    "# item_ci_df.sort_values(by='err_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372fe0f-1e01-4829-bb3c-46ed8dfd2bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_ci_df[item_ci_df['mean'] < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2a9c2-58ed-4dbb-bf34-bfe9f645a6ab",
   "metadata": {},
   "source": [
    "## How does performance vary across tests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d004d9e-b723-4714-82b8-dcd31e83feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Reloading test-performance dataframe\")\n",
    "    test_ci_df = pd.read_csv(\"./dataframes/test_ci_df.csv\")\n",
    "    test_item_level_data = pd.read_csv(\"./dataframes/test_item_level_data.csv\")\n",
    "    # raise Error\n",
    "except:\n",
    "    print(\"Regenerating test-performance dataframe\")\n",
    "    test_item_level_data, test_ci_df = create_confidence_interval_df(\n",
    "        performance_df, \n",
    "        measure='is_correct',\n",
    "        id_col='question_image',\n",
    "        condition_col='test_type',\n",
    "    )\n",
    "    test_item_level_data.to_csv(\"./dataframes/test_item_level_data.csv\", index=False)\n",
    "    test_ci_df.to_csv(\"./dataframes/test_ci_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ab218-5fac-4af9-9e33-8cf23dd43c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5aa02a-a2a4-4b8a-86bb-8b1733ad802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ci_df(original_ci_df, item_df):\n",
    "    ci_df = pd.merge(item_df[['item_id', 'chance_selection']], original_ci_df, left_on='item_id', right_on='category').copy()\n",
    "    ci_df['mean'] = ci_df['mean'] - ci_df['chance_selection']\n",
    "    ci_df['ci_upper'] = ci_df['ci_upper'] - ci_df['chance_selection']\n",
    "    ci_df['ci_lower'] = ci_df['ci_lower'] - ci_df['chance_selection']\n",
    "    return ci_df\n",
    "\n",
    "transform_ci_df(test_ci_df, item_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb3e58-faa9-4b80-9f4a-1ce1788995b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d6b0f4-4a45-4821-b602-6262db943825",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_order = list(assessment_color_map.keys())\n",
    "test_domain_colors = list(assessment_color_map.values())\n",
    "\n",
    "def create_test_accuracy_chart(ci_df, item_level_data, domain_order, test_domain_colors=[]):\n",
    "    \"\"\" Generate chart for point estimate and confidence intervals overlayed with item level scatteplots\n",
    "    \"\"\"\n",
    "    # Create dot+error bar plot\n",
    "    chart = alt.Chart(ci_df).mark_point(filled=True, size=75, opacity=1).encode(\n",
    "        x=alt.X('category:N', title='Category', scale=alt.Scale(domain=domain_order)),\n",
    "        y=alt.Y('mean:Q', title='Prop. Correct'),\n",
    "        color=alt.Color('category:N', scale=alt.Scale(domain=domain_order, range=test_domain_colors), legend=None),\n",
    "    ) + alt.Chart(ci_df).mark_rule(strokeWidth=2).encode(\n",
    "        x=alt.X('category:N'),\n",
    "        y=alt.Y('ci_lower:Q'),\n",
    "        y2='ci_upper:Q',\n",
    "        color=alt.Color('category:N', legend=None),\n",
    "        opacity=alt.value(1)\n",
    "    )\n",
    "    \n",
    "    scatter_plot = alt.Chart(item_level_data).mark_point(filled=True).encode(\n",
    "        x=alt.X('category:N', title='Category'),\n",
    "        y=alt.Y(\"is_correct:Q\",),\n",
    "        xOffset=\"jitter:Q\",\n",
    "        color=alt.Color('category:N', legend=None),\n",
    "        size=alt.value(16),\n",
    "        opacity=alt.value(0.3)\n",
    "    )\n",
    "    return (chart + scatter_plot)\n",
    "\n",
    "test_item_level_data['jitter'] = test_item_level_data.apply(\n",
    "    lambda _ : box_muller(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "test_type_performance = create_test_accuracy_chart(\n",
    "    test_ci_df, test_item_level_data, test_order, test_domain_colors\n",
    ").properties(\n",
    "    width=40*5, height=200, title=f\"Prop. correct across tests\", \n",
    ")\n",
    "test_type_performance.save(\"./figures/test_type_performance.pdf\")\n",
    "test_type_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c90dcd-9cd5-4689-b5fc-ff1480c9181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ci_df.sort_values(by=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33ad0b-5eaf-4b9c-9c85-5b3bc90cc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_level_data.groupby('category')['is_correct'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb02b5-3999-40b5-8550-9c7d88a1eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_chances = []\n",
    "for i, row in item_df[item_df['test_type'] == 'calvi'].iterrows():\n",
    "    valid_mc_counter = 0\n",
    "    for i in range(1, 11):\n",
    "        \n",
    "        if not pd.isna(row[f'mc{i}']):\n",
    "            valid_mc_counter += 1\n",
    "\n",
    "    total_chances.append(1 / valid_mc_counter)\n",
    "np.mean(total_chances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a15975-063d-4ed6-97f3-059edd53fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "item_means = performance_df.groupby([\"test_type\", 'question_image'])['is_correct'].mean().reset_index()\n",
    "\n",
    "groups = [performance_df[\"is_correct\"].values for _, group in item_means.groupby('test_type')]\n",
    "f_statistic, p_value = stats.f_oneway(*groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28de1dda-c6fb-48ee-99e7-bff91facb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8fc2f8-1784-4ddc-bcd6-fd0c60cf593b",
   "metadata": {},
   "source": [
    "## How does performance vary across different types of graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7ff9c-7773-4385-b3fb-2a43ccbdaedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Reloading test-performance dataframe\")\n",
    "    graph_item_level_data = pd.read_csv(\"./dataframes/graph_item_level_data.csv\")\n",
    "    graph_ci_df = pd.read_csv(\"./dataframes/graph_ci_df.csv\")\n",
    "except:\n",
    "    print(\"Regenerating test-performance dataframe\")\n",
    "    \n",
    "    def add_brbf_categories(r):\n",
    "        if (r['test_type'] == 'brbf'):\n",
    "            return '-'.join(r['image_file'].split(\"-\")[:-1])\n",
    "        return r['chart_type']\n",
    "\n",
    "    chart_performance_df = performance_df.copy()\n",
    "    chart_performance_df['chart_type_filled'] = chart_performance_df.apply(add_brbf_categories, axis=1)\n",
    "    chart_performance_df['common_chart_type'] = chart_performance_df['chart_type_filled'].replace(chart_categories)\n",
    "    chart_performance_df = chart_performance_df[chart_performance_df['common_chart_type'] != 'Table']\n",
    "    \n",
    "    graph_item_level_data, graph_ci_df = create_confidence_interval_df(\n",
    "        chart_performance_df, \n",
    "        measure='is_correct',\n",
    "        id_col='question_image',\n",
    "        condition_col='common_chart_type',\n",
    "    )\n",
    "    graph_item_level_data = pd.merge(graph_item_level_data, item_df[['test_type', 'question_image']])\n",
    "    graph_ci_df.to_csv(\"./dataframes/graph_ci_df.csv\",index=False)\n",
    "    graph_item_level_data.to_csv(\"./dataframes/graph_item_level_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79ff4c-b556-4cfa-adde-cd49f1158e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_order = list(assessment_color_map.keys())\n",
    "test_domain_colors = list(assessment_color_map.values())\n",
    "chart_order = graph_ci_df.sort_values(by=\"mean\", ascending=False)['category'].to_list()\n",
    "\n",
    "def format_hexcode_alpha(hexcode):\n",
    "    samples = np.linspace(60, 99, num=len(chart_order))\n",
    "    return [hexcode + str(int(s)) for s in samples][::-1]\n",
    "\n",
    "# color_order = [hexcode for hexcode in format_hexcode_alpha(\"#17202a\")]\n",
    "color_order = [\"#17202a\" for _ in range(len(chart_order))]\n",
    "\n",
    "def create_graph_performance_chart(ci_df, item_level_data, domain_order, domain_color=[]):\n",
    "    \"\"\" Generate chart for point estimate and confidence intervals overlayed with item level scatteplots\n",
    "    \"\"\"\n",
    "    # Create dot+error bar plot\n",
    "    chart = alt.Chart(ci_df).mark_point(filled=True, size=75, opacity=1).encode(\n",
    "        x=alt.X('category:N', title='Category', scale=alt.Scale(domain=domain_order)),\n",
    "        y=alt.Y('mean:Q', title='Prop. Correct'),\n",
    "        color=alt.Color('category:N', scale=alt.Scale(domain=domain_order, range=domain_color), legend=None),\n",
    "    ) + alt.Chart(ci_df).mark_rule(strokeWidth=2).encode(\n",
    "        x=alt.X('category:N'),\n",
    "        y=alt.Y('ci_lower:Q'),\n",
    "        y2='ci_upper:Q',\n",
    "        color=alt.Color('category:N', scale=alt.Scale(domain=domain_order, range=domain_color), legend=None)\n",
    "    )\n",
    "    \n",
    "    scatter_plot = alt.Chart(item_level_data).mark_point(opacity=0.35, filled=True).encode(\n",
    "        x=alt.X('category:N', title='Category'),\n",
    "        y=alt.Y(\"is_correct:Q\",),\n",
    "        xOffset=\"jitter:Q\",\n",
    "        color=alt.Color('test_type:N', scale=alt.Scale(domain=test_order, range=test_domain_colors), legend=None),\n",
    "        size=alt.value(16),\n",
    "    )\n",
    "\n",
    "    return (chart + scatter_plot).resolve_scale(color='independent')\n",
    "\n",
    "\n",
    "graph_item_level_data['jitter'] = graph_item_level_data.apply(\n",
    "    lambda _ : box_muller(),\n",
    "    axis=1\n",
    ")\n",
    "chart_performance = create_graph_performance_chart(graph_ci_df, graph_item_level_data, chart_order, color_order).properties(\n",
    "    width=13*34, height=200, title=f\"Prop. correct across tasks\", \n",
    ")\n",
    "chart_performance.save(\"./figures/graph_type_performance.pdf\")\n",
    "chart_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d93cac-0d4b-417d-942b-9a904d06d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_df = graph_item_level_data[['test_type', 'category']].value_counts().reset_index()\n",
    "\n",
    "# counted_df.pivot(values='count', index='category', columns='test_type')\n",
    "counted_df.groupby('category')['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd64da3-12da-49fa-85ac-30b69ba75271",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_ci_df.sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345cda1-68f1-4191-ac3d-b530d4f9cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_item_level_data.groupby('category')['is_correct'].std().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c03dd3-611c-4f34-887e-aaa902e8d543",
   "metadata": {},
   "source": [
    "## How does performance vary across different kinds of tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb8861-250c-4f60-a7e7-1fc4064db475",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Reloading task-performance dataframe\")\n",
    "    task_item_level_data = pd.read_csv(\"./dataframes/task_item_level_data.csv\")\n",
    "    task_ci_df = pd.read_csv(\"./dataframes/task_ci_df.csv\")\n",
    "except:\n",
    "    print(\"Regenerating task-performance dataframe\")\n",
    "    task_performance_df = performance_df.copy()\n",
    "    task_performance_df['task_category'] = task_performance_df['task_category'].apply(\n",
    "        lambda t : task_category_map[t] if t in task_category_map.keys() else t\n",
    "    )\n",
    "    task_performance_df['task_category'] = task_performance_df.apply(\n",
    "        lambda r : task_category_map[r['question']] if r['question'] in task_category_map.keys() else r['task_category'],\n",
    "        axis=1\n",
    "    )\n",
    "    # question_task_df = task_performance_df[['question_image', 'task_category']].value_counts().reset_index()\n",
    "    # question_task_df[['task_category']].value_counts().reset_index()\n",
    "    \n",
    "    task_item_level_data, task_ci_df = create_confidence_interval_df(\n",
    "        task_performance_df, \n",
    "        measure='is_correct',\n",
    "        id_col='question_image',\n",
    "        condition_col='task_category',\n",
    "    )\n",
    "    task_item_level_data = pd.merge(task_item_level_data, item_df[['test_type', 'question_image']])\n",
    "    task_item_level_data = task_item_level_data.reset_index()\n",
    "\n",
    "    task_item_level_data.to_csv(\"./dataframes/task_item_level_data.csv\", index=False)\n",
    "    task_ci_df.to_csv(\"./dataframes/task_ci_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232190f-a717-4470-9549-2501dacf1f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42bd51-5811-4c69-96c4-462feb90be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_performance_df = performance_df.copy()\n",
    "task_performance_df['task_category'] = task_performance_df['task_category'].apply(\n",
    "    lambda t : task_category_map[t] if t in task_category_map.keys() else t\n",
    ")\n",
    "task_performance_df['task_category'] = task_performance_df.apply(\n",
    "    lambda r : task_category_map[r['question']] if r['question'] in task_category_map.keys() else r['task_category'],\n",
    "    axis=1\n",
    ")\n",
    "task_performance_df[['test_type', 'task_category']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef9a7f-599a-4192-b59e-3dcddfb7ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_order = task_ci_df.sort_values(by=\"mean\", ascending=False)['category'].to_list()\n",
    "test_order = list(assessment_color_map.keys())\n",
    "test_domain_colors = list(assessment_color_map.values())\n",
    "task_color_order = [\"#17202a\" for _ in range(len(task_order))]\n",
    "\n",
    "def create_task_performance_chart(ci_df, item_level_data, domain_order, domain_color=[]):\n",
    "    \"\"\" Generate chart for point estimate and confidence intervals overlayed with item level scatteplots\n",
    "    \"\"\"\n",
    "    # Create dot+error bar plot\n",
    "    chart = alt.Chart(ci_df).mark_point(filled=True, size=75).encode(\n",
    "        x=alt.X('category:N', title='Category', scale=alt.Scale(domain=domain_order)),\n",
    "        y=alt.Y('mean:Q', title='Prop. Correct'),\n",
    "        color=alt.Color('category:N', scale=alt.Scale(domain=domain_order, range=domain_color), legend=None),\n",
    "        opacity=alt.value(1)\n",
    "    ) + alt.Chart(ci_df).mark_rule(strokeWidth=2).encode(\n",
    "        x=alt.X('category:N'),\n",
    "        y=alt.Y('ci_lower:Q'),\n",
    "        y2='ci_upper:Q',\n",
    "        color=alt.Color('category:N', scale=alt.Scale(domain=domain_order, range=domain_color), legend=None),\n",
    "        opacity=alt.value(1)\n",
    "    )\n",
    "\n",
    "    scatter_plot = alt.Chart(item_level_data).mark_point(filled=True).encode(\n",
    "        x=alt.X('category:N', title='Category'),\n",
    "        y=alt.Y(\"is_correct:Q\",),\n",
    "        xOffset=\"jitter:Q\",\n",
    "        color=alt.Color('test_type:N', scale=alt.Scale(domain=test_order, range=test_domain_colors), legend=None),\n",
    "        size=alt.value(16),\n",
    "        opacity=alt.value(0.3)\n",
    "    )    \n",
    "    return (scatter_plot + chart).resolve_scale(color='independent')\n",
    "\n",
    "task_item_level_data['jitter'] = task_item_level_data.apply(\n",
    "    lambda i : box_muller(),\n",
    "    axis=1\n",
    ")\n",
    "task_performance = create_task_performance_chart(\n",
    "    task_ci_df, task_item_level_data, task_order, task_color_order).properties(\n",
    "    width=40*3, height=200, title=f\"Prop. correct across tasks\", \n",
    ")\n",
    "task_performance.save(\"./figures/task_type_performance.pdf\")\n",
    "task_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e810c-4119-4eea-88ca-2f94054470e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73d60f-3499-4d60-9ee7-eb34c67936b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_item_level_data.groupby('category')['is_correct'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c87187-62d0-4b78-b54f-a8d6c0e3fa6e",
   "metadata": {},
   "source": [
    "## How does performance vary across presentation modality (tables vs. plots)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5215fd-06f5-4c2b-879f-c951edff6b91",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43d7e5-1cff-4ad0-9ed6-57b8465120fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_performance_df = performance_df.copy()\n",
    "modality_performance_df = modality_performance_df[\n",
    "    modality_performance_df['test_type'].isin(['brbf', 'wainer'])\n",
    "]\n",
    "\n",
    "modality_performance_df['chart_type'] = modality_performance_df.apply(\n",
    "    lambda r : r['graph_type'] if pd.isna(r['chart_type']) else r['chart_type'],\n",
    "    axis=1\n",
    ")\n",
    "modality_performance_df['chart_type'] = modality_performance_df['chart_type'].astype(str)\n",
    "modality_performance_df['modality'] = modality_performance_df.apply(\n",
    "    lambda r : \"table\" if \"table\" in r['chart_type'] else \"graph\",\n",
    "    axis=1\n",
    ")\n",
    "modality_question_count = modality_performance_df[['question_image', 'modality']].value_counts().reset_index()\n",
    "modality_count = modality_question_count[['modality']].value_counts().reset_index()\n",
    "modality_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5d020-fb5c-41d1-be28-ca7ff94efe2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7aebda-74ec-465d-9219-d3c0a03ceb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "wainer_gt_performance_df = modality_performance_df[\n",
    "    modality_performance_df['test_type'].isin(['wainer'])\n",
    "].copy()\n",
    "\n",
    "wainer_gt_performance_means = wainer_gt_performance_df.groupby(\n",
    "    ['question_image', 'question', 'chart_type', 'test_type', 'modality']\n",
    ")['is_correct'].mean().reset_index()\n",
    "wainer_gt_performance_means['question_id'] = wainer_gt_performance_means['chart_type']\n",
    "\n",
    "\n",
    "wainer_gt_performance_means_dfs = []\n",
    "for chart in ['bar', 'line', 'radial']:\n",
    "    wainer_gt_performance_chart_means = wainer_gt_performance_means[\n",
    "        wainer_gt_performance_means['chart_type'].isin(['table', chart])\n",
    "    ].copy()\n",
    "\n",
    "    wainer_gt_performance_chart_means['question_id'] = (\n",
    "        wainer_gt_performance_chart_means['question'] + chart\n",
    "    )\n",
    "    \n",
    "    wainer_gt_performance_means_dfs.append(\n",
    "        wainer_gt_performance_chart_means\n",
    "    )\n",
    "wainer_gt_performance_means = pd.concat(wainer_gt_performance_means_dfs)\n",
    "wainer_gt_performance_pivot = wainer_gt_performance_means.pivot(\n",
    "    index='question_id', values='is_correct', columns='modality'\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d096799-7ae8-4b16-b07d-011083219dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_category = [\n",
    "    ('bg-1', 'modified'),\n",
    "    ('bg-2', 'modified'),\n",
    "    ('bg-3', 'base'),\n",
    "    ('bg-4', 'base'),\n",
    "    ('bg-5', 'modified'),\n",
    "    ('bg-6', 'modified'),\n",
    "    ('bg-7', 'modified'),\n",
    "    ('bg-8', 'base'),\n",
    "    ('bg-9', 'modified'),\n",
    "    ('bg-10', 'base'),\n",
    "    ('bg-11', 'base'),\n",
    "    ('bg-12', 'base'),\n",
    "    ##\n",
    "    ('sp-1', 'modified'),\n",
    "    ('sp-2', 'base'),\n",
    "    ('sp-3', 'base'),\n",
    "    ('sp-4', 'base'),\n",
    "    ('sp-5', 'modified'),\n",
    "    ('sp-6', 'modified'),\n",
    "    ('sp-7', 'modified'),\n",
    "    ('sp-8', 'modified'),\n",
    "    ('sp-9', 'modified'),\n",
    "    ('sp-10', 'base'),\n",
    "    ('sp-11', 'base'),\n",
    "    ('sp-12', 'base'),\n",
    "    ##\n",
    "    ('lg1-1', 'base'),\n",
    "    ('lg1-2', 'modified'),\n",
    "    ('lg1-3', 'modified'),\n",
    "    ('lg1-4', 'base'),\n",
    "    ('lg1-5', 'modified'),\n",
    "    ('lg1-6', 'modified'),\n",
    "    ('lg1-7', 'base'),\n",
    "    ('lg1-8', 'base'),\n",
    "    ('lg1-9', 'base'),\n",
    "    ('lg1-10', 'modified'),\n",
    "    ('lg1-11', 'base'),\n",
    "    ('lg1-12', 'modified'),\n",
    "]\n",
    "\n",
    "for i in range(1, 13):\n",
    "    if i <= 6:\n",
    "        question_category.append((f'bg-table-{i}', \"base\"))\n",
    "        question_category.append((f'sp-table-{i}', \"base\"))\n",
    "        question_category.append((f'lg1-table-{i}', \"base\"))\n",
    "    else:\n",
    "        question_category.append((f'bg-table-{i}', \"modified\"))\n",
    "        question_category.append((f'sp-table-{i}', \"modified\"))\n",
    "        question_category.append((f'lg1-table-{i}', \"modified\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69056839-efac-4d36-88ad-8b9cca295830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055ccc1-413b-4508-b973-8bba530fd6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_category_df = pd.DataFrame(question_category, columns=['chart_type', 'item_type'])\n",
    "brbf_gt_performance_df = modality_performance_df[\n",
    "    modality_performance_df['test_type'].isin(['brbf'])\n",
    "].copy()\n",
    "\n",
    "brbf_gt_performance_df['chart_type'] = brbf_gt_performance_df['chart_type'].apply(\n",
    "    lambda c : c.replace('.png', '')\n",
    ")\n",
    "\n",
    "brbf_gt_performance_means = brbf_gt_performance_df.groupby(\n",
    "    ['question_image', 'task_category', 'chart_type', 'test_type', 'modality']\n",
    ")['is_correct'].mean().reset_index()\n",
    "\n",
    "brbf_gt_performance_means = pd.merge(brbf_gt_performance_means, modality_category_df)\n",
    "brbf_gt_performance_means['question_id'] = (\n",
    "    brbf_gt_performance_means['task_category'] + '-' +\n",
    "    brbf_gt_performance_means['item_type']\n",
    ")\n",
    "brbf_gt_performance_means['chart_type'] = brbf_gt_performance_means['chart_type'].apply(\n",
    "    lambda c : '-'.join(c.split('-')[:-1])\n",
    ")\n",
    "\n",
    "brbf_gt_performance_means['question_id'] = (\n",
    "    brbf_gt_performance_means['question_id'] + '-' \n",
    "    + brbf_gt_performance_means['chart_type'].apply(lambda c : c.replace(\"-table\", \"\"))\n",
    ")\n",
    "\n",
    "brbf_gt_performance_means_pivot = brbf_gt_performance_means.pivot(\n",
    "    index='question_id', columns='modality' , values='is_correct'\n",
    ").reset_index()\n",
    "\n",
    "wainer_gt_performance_pivot['test_type'] = 'wainer'\n",
    "brbf_gt_performance_means_pivot['test_type'] = 'brbf'\n",
    "\n",
    "table_graph_performance_means = pd.concat([\n",
    "    wainer_gt_performance_pivot,\n",
    "    brbf_gt_performance_means_pivot\n",
    "])\n",
    "table_graph_performance_means = table_graph_performance_means.melt(\n",
    "    id_vars=['question_id', 'test_type'], \n",
    "    value_vars=['graph', 'table']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1befd-c321-466c-8f5c-bc8208b4c7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1297e4-0c4d-4d17-9758-23707a023f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# item_level_data, ci_df = create_confidence_interval_df(\n",
    "#     modality_performance_df, \n",
    "#     measure='is_correct',\n",
    "#     id_col='question_image',\n",
    "#     condition_col='modality',\n",
    "# )\n",
    "\n",
    "\n",
    "try:\n",
    "    modality_ci_df = pd.read_csv(\"./dataframes/modality_item_level_data.csv\")\n",
    "    modality_item_level_data = pd.read_csv(\"./dataframes/modality_item_level_data.csv\")\n",
    "    table_graph_comparison_df = pd.read_csv(\"./dataframes/table_graph_comparison_df.csv\")\n",
    "    print(\"Reloading modality dataframes\")\n",
    "except:\n",
    "    print(\"Regenerating modality dataframes\")\n",
    "    mod_df = modality_performance_df.copy()\n",
    "    mod_df['task_category'] = mod_df['task_category'].apply(\n",
    "        lambda t : task_category_map[t] if t in task_category_map.keys() else t\n",
    "    )\n",
    "    mod_df['task_category'] = mod_df.apply(\n",
    "        lambda r : task_category_map[r['question']] if r['question'] in task_category_map.keys() else r['task_category'],\n",
    "        axis=1\n",
    "    )\n",
    "    mod_df['test_modality'] = mod_df['modality']  + ' - ' + mod_df['test_type']\n",
    "\n",
    "    \n",
    "    modality_item_level_data, modality_ci_df = create_confidence_interval_df(\n",
    "        mod_df,\n",
    "        measure='is_correct',\n",
    "        id_col='question_image',\n",
    "        condition_col='test_modality',\n",
    "    )\n",
    "    \n",
    "    modality_ci_df = modality_ci_df.rename(columns={'category': 'modality'})\n",
    "    modality_ci_df['test_type'] = modality_ci_df['modality'].apply(lambda m : m.split(\" - \")[1])\n",
    "    modality_ci_df['modality'] = modality_ci_df['modality'].apply(lambda m : m.split(\" - \")[0])\n",
    "\n",
    "    table_graph_comparison_df = table_graph_performance_means.pivot(\n",
    "        index='question_id', columns='modality' , values='value'\n",
    "    ).reset_index()\n",
    "    \n",
    "    modality_ci_df.to_csv(\"./dataframes/modality_ci_df.csv\")\n",
    "    modality_item_level_data.to_csv(\"./dataframes/modality_item_level_data.csv\")\n",
    "    table_graph_comparison_df.to_csv(\"./dataframes/table_graph_comparison_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578dd4a-a960-44cc-a537-fd21536ebfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modality_ci_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68189a03-af6b-4610-a8d8-c56a684b6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_graph_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395692bf-0d51-41c4-93fe-6d737c71e207",
   "metadata": {},
   "source": [
    "### Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e80abf-f488-4a9a-9410-a5c8bc2648e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_graph_performance_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd5d3b-8342-4bff-a4d4-f006183e7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "domain = ['table', 'graph']\n",
    "color_domain = [ assessment_color_map['wainer'], assessment_color_map['brbf']]\n",
    "test_domain = ['wainer', 'brbf']\n",
    "\n",
    "\n",
    "chart = alt.Chart(\n",
    "    table_graph_performance_means\n",
    ").mark_line(opacity=0.2).encode(\n",
    "    x='modality:N',\n",
    "    y='value',\n",
    "    detail='question_id',\n",
    "    color=alt.Color('test_type', scale=alt.Scale(domain=test_domain, range=color_domain),)\n",
    ")\n",
    "\n",
    "point_plot = alt.Chart(\n",
    "    table_graph_performance_means\n",
    ").mark_circle(opacity=0.1).encode(\n",
    "    x='modality:N',\n",
    "    y='value',\n",
    "    color=alt.Color('test_type',legend=None, scale=alt.Scale(domain=test_domain, range=color_domain)),\n",
    "    size=alt.value(30)\n",
    ")\n",
    "\n",
    "offset_domain = [30, 45]\n",
    "\n",
    "ci_point_plot =  alt.Chart(modality_ci_df).mark_line(strokeWidth=3, opacity=1).encode(\n",
    "    x=alt.X('modality:N', title='Category', scale=alt.Scale(domain=domain)),\n",
    "    y=alt.Y('mean:Q', title='Prop. Correct'),\n",
    "    opacity=alt.value(1),\n",
    "    xOffset=alt.XOffset('test_type', scale=alt.Scale(domain=test_domain, range=offset_domain)),\n",
    "    color=alt.Color(\n",
    "        'test_type', \n",
    "        scale=alt.Scale(domain=test_domain, range=color_domain),\n",
    "        legend=None\n",
    "    ),\n",
    ") + alt.Chart(modality_ci_df).mark_rule(strokeWidth=3, opacity=1).encode(\n",
    "    x=alt.X('modality:N'),\n",
    "    y=alt.Y('ci_lower:Q'),\n",
    "    y2='ci_upper:Q',\n",
    "    xOffset=alt.XOffset('test_type', scale=alt.Scale(domain=test_domain, range=offset_domain)),\n",
    "    color=alt.Color(\n",
    "        'test_type', \n",
    "        scale=alt.Scale(domain=test_domain, range=color_domain),\n",
    "        legend=None\n",
    "    ),\n",
    "    detail='modality'\n",
    ") + alt.Chart(modality_ci_df).mark_point(filled=True, size=75, fill='white', strokeWidth=2).encode(\n",
    "    x=alt.X('modality:N', title='Category', scale=alt.Scale(domain=domain)),\n",
    "    y=alt.Y('mean:Q', title='Prop. Correct'),\n",
    "    opacity=alt.value(1),\n",
    "    xOffset=alt.XOffset('test_type', scale=alt.Scale(domain=test_domain, range=offset_domain)),\n",
    "    stroke=alt.Color(\n",
    "        'test_type', \n",
    "        scale=alt.Scale(domain=test_domain, range=color_domain),\n",
    "        legend=None\n",
    "    ),\n",
    "    # detail='modality'\n",
    ")\n",
    "\n",
    "modality_plot = (chart + point_plot + ci_point_plot).resolve_scale(color='independent', xOffset='independent')\n",
    "modality_plot = modality_plot.properties(width=150, height=200, title=f\"Table vs Graph\")\n",
    "modality_plot.save(\"./figures/modality_performance.pdf\")\n",
    "modality_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d806c-d604-4d24-b527-c357f3a4e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modality_ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df5a4b-9fd4-4a91-98f7-2d1e0b06967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairwise_agent_heatmap(df, x, y, domain, units_of_measure, include_text=True, text_format=\".3f\"):\n",
    "\n",
    "    base = alt.Chart(df).mark_rect().encode(\n",
    "        x=alt.X(x, scale=alt.Scale(domain=domain)),  \n",
    "        y=alt.Y(y, scale=alt.Scale(domain=domain)),\n",
    "    )\n",
    "\n",
    "    color_domain=[-1,1]\n",
    "    color_reverse = False\n",
    "    color_condition = alt.condition(\n",
    "        alt.datum[units_of_measure] < 0.5,\n",
    "        alt.value('black'),\n",
    "        alt.value('white')\n",
    "    )\n",
    "\n",
    "\n",
    "    height=300\n",
    "    width=400 \n",
    "    title=f\"Accuracy Vector Correlation between Participants\"\n",
    "\n",
    "    heatmap = base.mark_rect().encode(\n",
    "        color=alt.Color(f'{units_of_measure}:Q', \n",
    "                        legend=None, \n",
    "                        scale=alt.Scale(\n",
    "                            scheme=\"redgrey\",\n",
    "                            domain=color_domain,\n",
    "                            reverse=color_reverse)\n",
    "                       ),\n",
    "    )\n",
    "\n",
    "    text = base.mark_text(baseline='middle').encode(\n",
    "        alt.Text(f'{units_of_measure}:Q', format=text_format),\n",
    "        color=color_condition\n",
    "    )\n",
    "\n",
    "    chart = heatmap.properties(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        title=title\n",
    "    ) \n",
    "    if include_text:\n",
    "        chart = chart + text\n",
    "\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4baaa02-747b-43a2-b606-d565a835ede9",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccda6c0-5d47-4fbe-a313-5ca491ebd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cbefb-2897-42de-a2e8-4cf78ceaf740",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_graph_comparison_df = pd.read_csv(\"./dataframes/table_graph_comparison_df.csv\")\n",
    "ttest_rel(table_graph_comparison_df['graph'], table_graph_comparison_df['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dfb0a8-4ae0-4e8f-9bae-bb51357d2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_graph_comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe44c43-62a6-4a69-899f-bcfafe8d7b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de80a022-3904-46b0-86ac-906fec059542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72d78ec5-c6d6-40bb-a333-b14ec84d5697",
   "metadata": {},
   "source": [
    "### Difference of means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cccf063-8508-4bed-8fca-b159d2806a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_diff_of_mean_ci(\n",
    "        data: pd.DataFrame,\n",
    "        measure,\n",
    "        id_col,\n",
    "        n_iterations=1000,\n",
    "        statistic=np.mean\n",
    "    ):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    items = list(data[id_col].unique())\n",
    "    n_size = len(items)\n",
    "    df = data.copy()\n",
    "\n",
    "    def bootstrap_iteration(data, chosen_items):\n",
    "        # filter_df = data[data[id_col].isin(chosen_items)] # Filter based on chosen questions\n",
    "        filter_df = pd.concat(\n",
    "            [data[data[id_col].isin([item])] for item in chosen_items]\n",
    "        )\n",
    "        \n",
    "        bs_mean = statistic(filter_df[measure]) \n",
    "        return (bs_mean, list(chosen_items))\n",
    "\n",
    "    \n",
    "    qset_means1 = Parallel(n_jobs=-1)(\n",
    "        delayed(bootstrap_iteration)(\n",
    "            df[df['modality'] == 'table'].copy(),\n",
    "            rng.choice(items, n_size,  replace=True)\n",
    "        ) for _ in range(n_iterations)\n",
    "    )\n",
    "\n",
    "    qset_means2 = Parallel(n_jobs=-1)(\n",
    "        delayed(bootstrap_iteration)(\n",
    "            df[df['modality'] == 'graph'].copy(),\n",
    "            rng.choice(items, n_size,  replace=True)\n",
    "        ) for _ in range(n_iterations)\n",
    "    )\n",
    "    \n",
    "    means = []\n",
    "    qs_used = []\n",
    "    means1 = [bs_mean for bs_mean, chosen_qs in qset_means1]\n",
    "    means2 = [bs_mean for bs_mean, chosen_qs in qset_means2]\n",
    "    means = np.subtract(means1, means2)\n",
    " \n",
    "    # 95% confidence interval\n",
    "    lower = np.percentile(means, 2.5)\n",
    "    upper = np.percentile(means, 97.5)\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "def create_diff_of_mean_ci_df(\n",
    "    data: pd.DataFrame,\n",
    "    measure,\n",
    "    id_col,\n",
    "    statistic=np.mean\n",
    "):\n",
    "    \"\"\" create the dataframe for 95% bootstrapped confidence interval\n",
    "    data: dataset\n",
    "    measure: dependent variable\n",
    "    id_col: units to bootstrap along (e.g. participant_d or item_id)\n",
    "    condition_col: the different conditions of the experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    data_list = []\n",
    "\n",
    "\n",
    "    lower, upper = bootstrap_diff_of_mean_ci(data, measure=measure, statistic=statistic, id_col=id_col)\n",
    "    mean1 = data[data['modality'] == 'table'][measure].mean()\n",
    "    mean2 = data[data['modality'] == 'graph'][measure].mean()\n",
    "    mean = mean1 - mean2\n",
    "    \n",
    "    data_list.append({\n",
    "        \"mean\": mean,\n",
    "        \"ci_upper\": upper, \n",
    "        \"ci_lower\": lower,\n",
    "    })\n",
    "\n",
    "    # item_level_data = data.rename(columns={\n",
    "    #     condition_col: 'category', \n",
    "    # }).groupby([id_col, 'category'])[measure].mean().reset_index()\n",
    "\n",
    "    return pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29146d17-7668-4f3a-a532-ab3ad37b70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_of_mean_modality_ci_df = create_diff_of_mean_ci_df(\n",
    "    mod_df[mod_df['test_type'] == 'brbf'],\n",
    "    measure='is_correct',\n",
    "    id_col='question_image',\n",
    ")\n",
    "diff_of_mean_modality_ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc5aca-cfee-4411-adca-d27a9eaf618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_of_mean_modality_ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bd27a-1107-4940-9b08-fad897fe3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_of_mean_modality_ci_df = create_diff_of_mean_ci_df(\n",
    "    mod_df[mod_df['test_type'] == 'wainer'],\n",
    "    measure='is_correct',\n",
    "    id_col='question_image',\n",
    ")\n",
    "diff_of_mean_modality_ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cdade-64d9-4684-97ad-1e84ad171be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mod_df['test_type'].unique()\n",
    "# mod_df[mod_df['test_type'] == 'brbf']['is_correct'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59339b9-6e30-40f6-a303-ba354fa32b8e",
   "metadata": {},
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7692e41e-1dd0-4862-beb2-42db5c569408",
   "metadata": {},
   "source": [
    "# Model comparison fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5847ca-e52d-4873-a9b2-a5faa0e6371c",
   "metadata": {},
   "source": [
    "## Load R + Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8b379-0f09-478e-a052-807b95b4a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32430a0-9172-4963-9960-f63354870977",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages(\"lme4\")\n",
    "library(lme4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980306b7-0e19-44e5-b9de-8a456125d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages(\"MuMIn\")\n",
    "library(MuMIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0faea-6f2e-4d97-be30-715c577d0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages(\"dplyr\")\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03346e49-b5dc-4416-b9d8-da87a3e6ba7b",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470e6be-6689-4b51-b81a-475f981bf303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bec589-3883-4c9d-95f1-b55e96a23211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0493686b-cbe6-4d1b-8d22-1a7bb79b7dc9",
   "metadata": {},
   "source": [
    "## Create dataframe for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5b34a-c7bc-41fa-810c-9b6ca948664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_brbf_categories(r):\n",
    "    if (r['test_type'] == 'brbf'):\n",
    "        return '-'.join(r['image_file'].split(\"-\")[:-1])\n",
    "    return r['chart_type']\n",
    "\n",
    "\n",
    "all_df = performance_df.copy()\n",
    "all_df['chart_type_filled'] = all_df.apply(add_brbf_categories, axis=1)\n",
    "all_df['chart_type'] = all_df['chart_type_filled'].replace(chart_categories)\n",
    "all_df = all_df[all_df['chart_type'] != 'Table']\n",
    "\n",
    "all_df['task_category'] = all_df['task_category'].apply(\n",
    "    lambda t : task_category_map[t] if t in task_category_map.keys() else t\n",
    ")\n",
    "all_df['task_category'] = all_df.apply(\n",
    "    lambda r : task_category_map[r['question']] if r['question'] in task_category_map.keys() else r['task_category'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "all_df['item_id'] = all_df['question_image']\n",
    "all_df = all_df[['test_type', 'chart_type', 'task_category', 'item_id', 'is_correct']]\n",
    "initial_df = all_df.copy()\n",
    "\n",
    "print(\n",
    "    all_df['chart_type'].unique(), \n",
    "    all_df['task_category'].unique(), \n",
    "    all_df['test_type'].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7dd4f8-c207-4707-a7e2-92caea75c86b",
   "metadata": {},
   "source": [
    "## Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e2447e-b11d-4e37-af1c-863cb9350ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# define options for all models\n",
    "control <- glmerControl(\n",
    "    optimizer = \"bobyqa\",\n",
    "    optCtrl = list(maxfun = 2e5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098baadc-dd5c-4b18-8024-4c19dddcd215",
   "metadata": {},
   "source": [
    "### Null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8ed61-8479-4866-abfa-0b1663fc7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "null_model <- glmer(\n",
    "    is_correct ~ 1 + (1 | item_id), \n",
    "    data = all_df, \n",
    "    family = binomial, \n",
    "    control = control\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6eeca-9f15-428d-ac3e-b17358dac82d",
   "metadata": {},
   "source": [
    "### Comparing test fixed effect to null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d9d44-e517-4623-97a5-13e4bf2f1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "test_model <- glmer(\n",
    "    is_correct ~ test_type + (1 | item_id), \n",
    "    data = all_df, \n",
    "    family = binomial,\n",
    "    control = control\n",
    ")\n",
    "\n",
    "anova(null_model, test_model, test=\"LRT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e86056-f74a-42a3-ab72-8ad1070775f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "r.squaredGLMM(test_model, null_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19168f6b-03e9-4673-bc03-05525ad64826",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Comparing task type fixed effect to null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572d876-027b-4686-934f-76173c34c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "task_model <- glmer(\n",
    "    is_correct ~ task_category + (1 | item_id), \n",
    "    data = all_df, \n",
    "    family = binomial, \n",
    "    control = control\n",
    ")\n",
    "\n",
    "anova(null_model, task_model, test=\"LRT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e944fb-4dea-47d8-970d-ef17b5592b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3a9b92-0487-4404-bca9-686e81d4b6a1",
   "metadata": {},
   "source": [
    "### Comparing graph type fixed effect to null model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87ca37-540c-4dd1-8fd3-72e3924b17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "control <- glmerControl(\n",
    "    optimizer = \"bobyqa\",\n",
    "    optCtrl = list(maxfun = 2e5)\n",
    ")\n",
    "\n",
    "graph_model <- glmer(\n",
    "    is_correct ~ chart_type + (1 | item_id), \n",
    "    data = all_df, \n",
    "    family = binomial,\n",
    "    control = control\n",
    ")\n",
    "\n",
    "anova(null_model, graph_model, test=\"LRT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c8a92-ee12-4466-b55e-3598fc3e239b",
   "metadata": {},
   "source": [
    "### Comparing graph/test type fixed effect to item model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f935cce-9956-48fe-a14d-d88c191d4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "control <- glmerControl(\n",
    "    optimizer = \"bobyqa\",\n",
    "    optCtrl = list(maxfun = 2e5)\n",
    ")\n",
    "\n",
    "task_model <- glmer(\n",
    "    is_correct ~ task_category + (1 | item_id), \n",
    "    data = all_df, \n",
    "    family = binomial,\n",
    "    control = control\n",
    ")\n",
    "\n",
    "task_graph_model <- glmer(\n",
    "    is_correct ~ task_category + chart_type + (1 | item_id), \n",
    "    data = all_df, \n",
    "    family = binomial,\n",
    "    control = control\n",
    ")\n",
    "\n",
    "task_test_model <- glmer(\n",
    "    is_correct ~ test_type + task_category + (1 | item_id), \n",
    "    data = all_df, \n",
    "    family = binomial,\n",
    "    control = control\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0db5de-3000-4b29-a733-2e94b7889d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "anova(task_model, task_graph_model, test=\"LRT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf2dcd-a908-421b-8726-8610b4b3ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "anova(task_model, task_test_model, test=\"LRT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d55d4e-a81a-4445-8d2a-f6983154a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3643cb-2143-43b3-a2b0-f29fd60d9ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09b8a2a3-7745-4ab9-a772-5e314a1a9629",
   "metadata": {},
   "source": [
    "### Find model fit + variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f72f76-d032-4ab7-b3f7-7740246ba33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "control <- glmerControl(\n",
    "    optimizer = \"bobyqa\",\n",
    "    optCtrl = list(maxfun = 2e5)\n",
    ")\n",
    "\n",
    "# Logistic regression with item as a fixed effect\n",
    "null_model <- glmer(is_correct ~ 1 + (1 | item_id), data = all_df, family = binomial, control = control)\n",
    "test_model <- glmer(is_correct ~ test_type + (1 | item_id), data = all_df, family = binomial, control = control)\n",
    "task_model <- glmer(is_correct ~ task_category + (1 | item_id), data = all_df, family = binomial, control = control)\n",
    "graph_model <- glmer(is_correct ~ chart_type + (1 | item_id), data = all_df, family = binomial, control = control)\n",
    "graph_test_model <- glmer(is_correct ~ test_type + chart_type + (1 | item_id), data = all_df, family = binomial, control = control)\n",
    "graph_task_model <- glmer(is_correct ~ task_category + chart_type + (1 | item_id), data = all_df, family = binomial, control = control)\n",
    "task_test_model <- glmer(is_correct ~ test_type + task_category + (1 | item_id), data = all_df, family = binomial, control = control)\n",
    "graph_task_test_model <- glmer(is_correct ~ test_type + task_category + chart_type + (1 | item_id), data = all_df, family = binomial, control = control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd08bba-844b-4769-a698-f76ac1354874",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "control <- glmerControl(\n",
    "    optimizer = \"bobyqa\",\n",
    "    optCtrl = list(maxfun = 2e5)\n",
    ")\n",
    "\n",
    "interaction_model <- glmer(\n",
    "    is_correct ~ test_type*task_category + chart_type + (1 | item_id), \n",
    "    data = all_df, family = binomial,\n",
    "    control = control\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74b3f2-5d04-4d37-90ba-8886d538e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "results <- data.frame(\n",
    "  Model = character(),\n",
    "  Marginal_R2 = numeric(),\n",
    "  Conditional_R2 = numeric(),\n",
    "  BIC = numeric(),\n",
    "  AIC = numeric(),\n",
    "  bootstap_iter = numeric(),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "# List of models with their formulas\n",
    "model_formulas <- list(\n",
    "null_model = is_correct ~ (1 | item_id),\n",
    "test_model = is_correct ~ test_type + (1 | item_id),\n",
    "task_model = is_correct ~ task_category + (1 | item_id),\n",
    "graph_model = is_correct ~ chart_type + (1 | item_id),\n",
    "graph_test_model = is_correct ~ test_type + chart_type + (1 | item_id),\n",
    "graph_task_model = is_correct ~ task_category + chart_type + (1 | item_id),\n",
    "task_test_model = is_correct ~ test_type + task_category + (1 | item_id),\n",
    "graph_task_test_model = is_correct ~ test_type + task_category + chart_type + (1 | item_id),\n",
    "interaction_model = is_correct ~ test_type * task_category + chart_type + (1 | item_id)\n",
    ")\n",
    "\n",
    "for (model_name in names(model_formulas)) {\n",
    "formula <- model_formulas[[model_name]]\n",
    "warning_occurred <- FALSE\n",
    "\n",
    "model <- tryCatch({\n",
    "  fit <- glmer(\n",
    "    formula,\n",
    "    data = all_df,\n",
    "    family = binomial,\n",
    "    control = control\n",
    "  )\n",
    "  fit\n",
    "}, warning = function(w) {\n",
    "  if (grepl(\"fixed-effect model matrix is rank deficient\", w$message)) {\n",
    "    warning_occurred <<- TRUE\n",
    "  }\n",
    "  invokeRestart(\"muffleWarning\")\n",
    "})\n",
    "\n",
    "# Skip to the next iteration if model fitting failed\n",
    "if (inherits(model, \"error\")) next\n",
    "\n",
    "# Compute R-squared, BIC, and AIC\n",
    "r_squared <- r.squaredGLMM(model, null_model)\n",
    "bic_value <- BIC(model)\n",
    "aic_value <- AIC(model)\n",
    "\n",
    "# Append results to the data frame\n",
    "results <- rbind(results, data.frame(\n",
    "  Model = model_name,\n",
    "  Marginal_R2 = r_squared[1],\n",
    "  Conditional_R2 = r_squared[2],\n",
    "  BIC = bic_value,\n",
    "  AIC = aic_value,\n",
    "  bootstap_iter = i,\n",
    "  Warning = warning_occurred  # Store whether warning occurred\n",
    "))\n",
    "}\n",
    "\n",
    "write.csv(results, \"./dataframes/model_fit_fulldata.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060d5ed-2acb-4d09-9ebd-9a502e4fe699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%R\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baccd77-9a32-4e53-a313-f7651b868221",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = all_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65d49e-27be-4933-aaa4-d261b0dc9581",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(initial_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657189d5-b751-4640-b189-d08ffccb10b6",
   "metadata": {},
   "source": [
    "### Bootstrap model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ab1c4-3ede-400e-8716-8f56bc0ccf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i initial_df\n",
    "\n",
    "control <- glmerControl(\n",
    "    optimizer = \"bobyqa\",\n",
    "    optCtrl = list(maxfun = 2e5)\n",
    ")\n",
    "\n",
    "unique_items <- unique(initial_df$item_id)\n",
    "\n",
    "# Initialize an empty data frame to store the results\n",
    "results <- data.frame(\n",
    "  Model = character(),\n",
    "  Marginal_R2 = numeric(),\n",
    "  Conditional_R2 = numeric(),\n",
    "  BIC = numeric(),\n",
    "  AIC = numeric(),\n",
    "  bootstap_iter = numeric(),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "all_sampled_items <- list()\n",
    "\n",
    "for (i in 1:100) {\n",
    "  print(i)\n",
    "  sampled_items <- sample(unique_items, size = length(unique_items), replace = TRUE)\n",
    "  all_sampled_items <- c(all_sampled_items, list(sampled_items))\n",
    "\n",
    "  all_df <- do.call(rbind, lapply(sampled_items, function(item) {\n",
    "    initial_df[initial_df$item_id == item, ]\n",
    "  }))\n",
    "\n",
    "  null_model <- glmer(\n",
    "    is_correct ~ 1 + (1 | item_id), \n",
    "    data = all_df, \n",
    "    family = binomial, \n",
    "    control = control\n",
    "  )\n",
    "\n",
    "  # List of models with their formulas\n",
    "  model_formulas <- list(\n",
    "    null_model = is_correct ~ (1 | item_id),\n",
    "    test_model = is_correct ~ test_type + (1 | item_id),\n",
    "    task_model = is_correct ~ task_category + (1 | item_id),\n",
    "    graph_model = is_correct ~ chart_type + (1 | item_id),\n",
    "    graph_test_model = is_correct ~ test_type + chart_type + (1 | item_id),\n",
    "    graph_task_model = is_correct ~ task_category + chart_type + (1 | item_id),\n",
    "    task_test_model = is_correct ~ test_type + task_category + (1 | item_id),\n",
    "    graph_task_test_model = is_correct ~ test_type + task_category + chart_type + (1 | item_id),\n",
    "    interaction_model = is_correct ~ test_type * task_category + chart_type + (1 | item_id)\n",
    "  )\n",
    "\n",
    "  for (model_name in names(model_formulas)) {\n",
    "    formula <- model_formulas[[model_name]]\n",
    "    warning_occurred <- FALSE\n",
    "\n",
    "    model <- tryCatch({\n",
    "      fit <- glmer(\n",
    "        formula,\n",
    "        data = all_df,\n",
    "        family = binomial,\n",
    "        control = control\n",
    "      )\n",
    "      fit\n",
    "    }, warning = function(w) {\n",
    "      if (grepl(\"fixed-effect model matrix is rank deficient\", w$message)) {\n",
    "        warning_occurred <<- TRUE\n",
    "      }\n",
    "      invokeRestart(\"muffleWarning\")\n",
    "    })\n",
    "\n",
    "    # Skip to the next iteration if model fitting failed\n",
    "    if (inherits(model, \"error\")) next\n",
    "\n",
    "    # Compute R-squared, BIC, and AIC\n",
    "    r_squared <- r.squaredGLMM(model, null_model)\n",
    "    bic_value <- BIC(model)\n",
    "    aic_value <- AIC(model)\n",
    "\n",
    "    # Append results to the data frame\n",
    "    results <- rbind(results, data.frame(\n",
    "      Model = model_name,\n",
    "      Marginal_R2 = r_squared[1],\n",
    "      Conditional_R2 = r_squared[2],\n",
    "      BIC = bic_value,\n",
    "      AIC = aic_value,\n",
    "      bootstap_iter = i,\n",
    "      Warning = warning_occurred  # Store whether warning occurred\n",
    "    ))\n",
    "  }\n",
    "}\n",
    "\n",
    "write.csv(results, \"./figures/model_statistics2.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f923ee-4299-4875-9a68-899438786791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "my_list <- all_sampled_items\n",
    "num_columns <- max(sapply(my_list, length))\n",
    "column_names <- paste0(\"Column_\", seq_len(num_columns))\n",
    "\n",
    "df <- do.call(rbind, lapply(my_list, function(x) {\n",
    "  length(x) <- num_columns\n",
    "  x\n",
    "}))\n",
    "\n",
    "# Assign column names to the dataframe\n",
    "colnames(df) <- column_names\n",
    "\n",
    "# Convert to a data.frame and ensure appropriate types\n",
    "df <- as.data.frame(df, stringsAsFactors = FALSE)\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "write.csv(df, \"./model_bootstrapped_items.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0513d2-4274-4563-ab27-8c03edf19442",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bootstrapped_items = pd.read_csv(\"model_bootstrapped_items.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eda3d5-a8d9-4aed-8d91-b240c4e50617",
   "metadata": {},
   "source": [
    "### Plot Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f7dae-222b-4a5c-ba64-81791d561aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_interaction_combos = all_df[['item_id', 'test_type', 'task_category']].drop_duplicates()\n",
    "valid_interaction_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5c912-ee78-4b80-8c72-bc4ec53e0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87feda1d-a686-4d0b-a388-0d7fc869d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_interaction_runs = []\n",
    "for i, row in pd.read_csv(\"./model_bootstrapped_items.csv\").iterrows():\n",
    "    item = pd.DataFrame({'item_id': row.to_list()}).drop_duplicates()\n",
    "    test_task_interaction_counts = pd.merge(valid_interaction_combos, item)[['test_type', 'task_category']].value_counts()\n",
    "    test_task_interaction_counts = test_task_interaction_counts.reset_index()\n",
    "    test_task_interaction_counts = test_task_interaction_counts.pivot(index='test_type', columns='task_category', values='count')\n",
    "\n",
    "    if (len(test_task_interaction_counts) != len(test_task_interaction_counts.dropna())):\n",
    "        invalid_interaction_runs.append(i)\n",
    "        print(\"Invalid bootstrap for interaction model found: Row \", i)\n",
    "print(\"Total runs invalid: \", len(invalid_interaction_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276225a-2601-4dab-a92f-545b04ec8f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_statistics_allbs = pd.read_csv(\"./figures/model_statistics2.csv\")\n",
    "model_statistics_allbs = model_statistics_allbs[~model_statistics_allbs['bootstap_iter'].isin(invalid_interaction_runs)]\n",
    "len(model_statistics_allbs['bootstap_iter'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c8fac-f1cf-4d73-b8c7-dd79e172950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_statistics_allbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbadf2-9f4f-43bc-8683-a7fabdb5fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_model = model_statistics_allbs[model_statistics_allbs['Model'] == 'null_model']\n",
    "\n",
    "def get_bootstrap_iter_aic(r):\n",
    "    aic = r['AIC']\n",
    "    null_aic = null_model[null_model['bootstap_iter'] == r['bootstap_iter']]['AIC'].iloc[0]\n",
    "    return aic - null_aic\n",
    "\n",
    "model_statistics_allbs['relative_aic'] = model_statistics_allbs.apply(\n",
    "    get_bootstrap_iter_aic,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7967bcf-167b-4dcf-bee7-d05547373167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1089689-6dac-4351-adb3-28cbb539f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_statistics = model_statistics_allbs.copy()\n",
    "model_statistics = model_statistics[model_statistics['Model'] != 'null_model']\n",
    "model_statistics_ci = model_statistics.groupby('Model').agg(\n",
    "    bic_mean=('BIC', 'mean'),\n",
    "    bic_std=('BIC', 'std'),\n",
    "    relative_aic_mean=('relative_aic', 'mean'),\n",
    "    relative_aic_std=('relative_aic', 'std'),\n",
    "    mr2_mean=('Marginal_R2', 'mean'),\n",
    "    mr2_std=('Marginal_R2', 'std'),\n",
    "    mr2_ci_upper=('Marginal_R2', lambda m : np.percentile(m, 97.5)),\n",
    "    mr2_ci_lower=('Marginal_R2', lambda m : np.percentile(m, 2.5 )),\n",
    "    cr2_mean=('Conditional_R2', 'mean'),\n",
    "    cr2_std=('Conditional_R2', 'std'),\n",
    ").reset_index()\n",
    "\n",
    "model_statistics_ci['mr2_ystd'] = model_statistics_ci['mr2_mean'] - model_statistics_ci['mr2_std']\n",
    "model_statistics_ci['mr2_ystd2'] = model_statistics_ci['mr2_mean'] + model_statistics_ci['mr2_std']\n",
    "\n",
    "model_statistics_ci['cr2_ystd'] = model_statistics_ci['cr2_mean'] - model_statistics_ci['cr2_std']\n",
    "model_statistics_ci['cr2_ystd2'] = model_statistics_ci['cr2_mean'] + model_statistics_ci['cr2_std']\n",
    "\n",
    "model_statistics_ci['relative_aic_ystd'] = model_statistics_ci['relative_aic_mean'] - model_statistics_ci['relative_aic_std']\n",
    "model_statistics_ci['relative_aic_ystd2'] = model_statistics_ci['relative_aic_mean'] + model_statistics_ci['relative_aic_std']\n",
    "\n",
    "# replace means with full data fit\n",
    "model_fit_on_fulldata = pd.read_csv(\"./dataframes/model_fit_fulldata.csv\")\n",
    "model_fit_on_fulldata = model_fit_on_fulldata.rename(columns={'Marginal_R2': 'mr2_fulldata'})\n",
    "model_statistics_ci = pd.merge(model_statistics_ci, model_fit_on_fulldata[['Model', 'mr2_fulldata']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f5c3f-1896-4257-8292-6772522b2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fit_on_fulldata\n",
    "model_statistics_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bade94-9eee-4f02-89c5-b8e467cd36da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fit_on_fulldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed2df5-39c5-40e3-8b5c-21b34aab0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_statistics_ci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74912d-82b9-48d5-ac96-1a7360fc99a1",
   "metadata": {},
   "source": [
    "#### Get Split-half R^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded727d-1876-4021-a772-73055cc69bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_half_correlations = []\n",
    "for _ in range(10000):\n",
    "    correlation_df = all_df.sample(frac=1, replace=False)\n",
    "    half_one = correlation_df.iloc[0:(len(all_df) // 2)].groupby('item_id')['is_correct']\n",
    "    half_one = half_one.mean().reset_index().sort_values(by='item_id')\n",
    "    \n",
    "    half_two = correlation_df.iloc[(len(all_df) // 2):len(all_df)].groupby('item_id')['is_correct']\n",
    "    half_two = half_two.mean().reset_index().sort_values(by='item_id')\n",
    "    \n",
    "    iter_corr = np.corrcoef(half_one['is_correct'], half_two['is_correct'])\n",
    "    split_half_correlations.append(iter_corr[0][1]**2)\n",
    "\n",
    "split_half_mean = np.mean(split_half_correlations)\n",
    "split_half_ci_lower, split_half_ci_upper = np.percentile(split_half_correlations, [2.5, 97.5])\n",
    "split_half_df = pd.DataFrame({\n",
    "    'index': [0],\n",
    "    'mean': split_half_mean,\n",
    "    'ci_upper': split_half_ci_upper,\n",
    "    'ci_lower': split_half_ci_lower,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4171f4f-f993-4eb3-9eaf-9e481da37c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_half_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d6d0d-c5fb-43da-8a0e-a94961cf7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_order = [\n",
    "    'test_model',\n",
    "    'task_model',\n",
    "    'graph_model',\n",
    "    'task_test_model',\n",
    "    'graph_test_model',\n",
    "    'graph_task_model',     \n",
    "    'graph_task_test_model',\n",
    "    'interaction_model'\n",
    "]\n",
    "\n",
    "line_path = \"M -3.5,0 L 3.5,0\"\n",
    "\n",
    "def plot_model_fit(measure, yscale):\n",
    "    mean_model_plot = alt.Chart(model_statistics_ci).mark_point(color='black', shape=line_path).encode(\n",
    "        x=alt.X('Model:N', scale=alt.Scale(domain=model_order)),\n",
    "        y=alt.Y(f'{measure}_fulldata:Q', scale=alt.Scale(domain=yscale), title=\"\"),\n",
    "        strokeWidth=alt.value(2)\n",
    "    )\n",
    "    \n",
    "    std_model_plot = alt.Chart(model_statistics_ci).mark_bar(color='black', opacity=0.1).encode(\n",
    "        x=alt.X('Model:N', scale=alt.Scale(domain=model_order)),\n",
    "        y=alt.Y(f'{measure}_ci_upper:Q'),\n",
    "        y2=(f'{measure}_ci_lower:Q'),\n",
    "        size=alt.value(20)\n",
    "    )\n",
    "\n",
    "    split_half_band = alt.Chart(split_half_df).mark_rect(color='black', opacity=0.2).encode(\n",
    "        y=alt.Y(f'ci_upper:Q'),\n",
    "        y2=(f'ci_lower:Q'),\n",
    "    )\n",
    "\n",
    "    split_half_mean = alt.Chart(split_half_df).mark_rule(color='black', strokeDash=[5, 5]).encode(\n",
    "        y=alt.Y(f'mean:Q'),\n",
    "        strokeWidth=alt.value(1)\n",
    "    )\n",
    "    \n",
    "    model_fit_figure = (mean_model_plot + std_model_plot + split_half_band + split_half_mean).properties(\n",
    "        height=200,\n",
    "        width=200\n",
    "    )\n",
    "\n",
    "    return model_fit_figure\n",
    "\n",
    "# mr2_fulldata\n",
    "model_fit_figure = plot_model_fit('mr2', yscale=[0, 1])\n",
    "model_fit_figure.save(\"./figures/model_fit.pdf\")\n",
    "model_fit_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efde6067-8daf-4aa4-90d7-6db59aadcdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_statistics_ci[['Model', 'mr2_fulldata', 'mr2_ci_upper', 'mr2_ci_lower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad948e-7093-4d25-b826-2fa324e20998",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = item_df[['test_type', 'task_category']].value_counts().reset_index()\n",
    "temp.pivot(index='test_type', columns='task_category', values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e14a39-141c-4a47-bc79-1f249413643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_statistics = pd.read_csv(\"./figures/model_statistics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08917c1-6ae9-419a-b1ba-820fa8f073dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df8dd6ef-5070-4f61-9a00-ead922ad45b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Cross validation across top-down categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99325a8c-f8e7-4cbe-aa1a-78b49245ded1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preparing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc1eba-7c68-45ef-abcb-96f9af7359a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_brbf_categories(r):\n",
    "    if (r['test_type'] == 'brbf'):\n",
    "        return '-'.join(r['image_file'].split(\"-\")[:-1])\n",
    "    return r['chart_type']\n",
    "\n",
    "\n",
    "all_df = performance_df.copy()\n",
    "all_df['chart_type_filled'] = all_df.apply(add_brbf_categories, axis=1)\n",
    "all_df['chart_type'] = all_df['chart_type_filled'].replace(chart_categories)\n",
    "all_df = all_df[all_df['chart_type'] != 'Table']\n",
    "\n",
    "all_df['task_category'] = all_df['task_category'].apply(\n",
    "    lambda t : task_category_map[t] if t in task_category_map.keys() else t\n",
    ")\n",
    "all_df['task_category'] = all_df.apply(\n",
    "    lambda r : task_category_map[r['question']] if r['question'] in task_category_map.keys() else r['task_category'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "all_df['item_id'] = all_df['question_image']\n",
    "all_df = all_df[['test_type', 'chart_type', 'task_category', 'item_id', 'is_correct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2efaf-e041-42a9-87b0-fc38c65f660a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf25668-a00c-41f4-a85c-0d088c08aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a2290-d24a-4e83-bb90-fb86ba33c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "install.packages(\"lme4\")\n",
    "install.packages(\"caret\")\n",
    "install.packages(\"dplyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da6701-73fe-4985-bc69-48c290317ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(lme4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597110b-79d5-4b37-b78c-9703c9c05401",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Cross-fold eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4c260-bc70-474b-8cc2-825a8e633966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_item_accuracy_chart(ci_df, domain_order):\n",
    "    \"\"\" Generate chart for point estimate and confidence intervals overlayed with item level scatteplots\n",
    "    \"\"\"\n",
    "    # Create dot+error bar plot\n",
    "    chart = alt.Chart(ci_df).mark_bar(opacity=0.7).encode(\n",
    "        x=alt.X('category:N', title='Category', scale=alt.Scale(domain=domain_order), axis=None),\n",
    "        y=alt.Y('mean:Q', title='Prop. Correct'),\n",
    "        color=alt.Color('test_type:N', scale=alt.Scale(domain=test_domain, range=test_domain_color), legend=None),\n",
    "    ) \n",
    "    # + alt.Chart(ci_df).mark_rule(strokeWidth=0.4, opacity=1).encode(\n",
    "    #     x=alt.X('category:N'),\n",
    "    #     y=alt.Y('ci_lower:Q'),\n",
    "    #     y2='ci_upper:Q',\n",
    "    #     color=alt.value(\"#717d7e\")\n",
    "    #     # color=alt.Color('test_type:N', scale=alt.Scale(domain=test_domain, range=test_domain_color), legend=None),\n",
    "    # )\n",
    "    \n",
    "    return chart.properties(width=1200, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22f178-c47b-4e18-a76d-be8540333482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d564475-f48a-4d6e-b66c-a87944dc82bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24042c40-02e8-4ef0-a3ba-ba64bbdcfa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(lme4)    # For mixed-effects models\n",
    "library(caret)   # For creating cross-validation folds\n",
    "library(dplyr)   # For data manipulation\n",
    "\n",
    "# Assume all_df is your dataset\n",
    "# all_df <- read.csv(\"your_data.csv\")\n",
    "\n",
    "# Define the model formula\n",
    "model_formula <- is_correct ~ test_type + task_category * chart_type + (1 | item_id)\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "set.seed(42)  # For reproducibility\n",
    "folds <- createFolds(all_df$is_correct, k = 5, list = TRUE, returnTrain = TRUE)\n",
    "\n",
    "# Initialize vectors to store results\n",
    "log_likelihoods <- numeric(length(folds))\n",
    "accuracies <- numeric(length(folds))\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for (i in seq_along(folds)) {\n",
    "  # Split the data into training and test sets\n",
    "  train_indices <- folds[[i]]\n",
    "  train_data <- all_df[train_indices, ]\n",
    "  test_data <- all_df[-train_indices, ]\n",
    "  \n",
    "  # Fit the model on the training data\n",
    "  model <- glmer(model_formula, data = train_data, family = binomial)\n",
    "  \n",
    "  # Predict probabilities on the test data\n",
    "  test_data$predicted <- predict(model, newdata = test_data, type = \"response\")\n",
    "  \n",
    "  # Convert probabilities to binary predictions (threshold = 0.5)\n",
    "  test_data$predicted_binary <- ifelse(test_data$predicted > 0.5, 1, 0)\n",
    "  \n",
    "  # Calculate log-likelihood for this fold\n",
    "  log_likelihoods[i] <- logLik(model)\n",
    "  \n",
    "  # Calculate accuracy for this fold\n",
    "  accuracies[i] <- mean(test_data$is_correct == test_data$predicted_binary)\n",
    "}\n",
    "\n",
    "# Summary of cross-validation results\n",
    "mean_log_likelihood <- mean(log_likelihoods)\n",
    "mean_accuracy <- mean(accuracies)\n",
    "\n",
    "# Print results\n",
    "cat(\"Mean Log-Likelihood:\", mean_log_likelihood, \"\\n\")\n",
    "cat(\"Mean Accuracy:\", mean_accuracy, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b1676-3f2f-4411-8a9d-f4e99e8a8123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e1151-b970-4a9d-8144-9f0f33396bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efefbd0a-761a-46d6-9077-41ba55c2fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "# Load your data (replace this with your actual DataFrame)\n",
    "# Example: all_df = pd.read_csv('your_data.csv')\n",
    "all_df = ...  # Your data here\n",
    "\n",
    "# Define the model formula\n",
    "model_formula = \"is_correct ~ test_type + task_category * chart_type + (1|item_id)\"\n",
    "\n",
    "# Initialize KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# To store results\n",
    "log_likelihoods = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for train_index, test_index in kf.split(all_df):\n",
    "    # Split the data into training and test sets\n",
    "    train_data = all_df.iloc[train_index]\n",
    "    test_data = all_df.iloc[test_index]\n",
    "    \n",
    "    # Fit the mixed-effects model on the training data\n",
    "    model = Lmer(model_formula, data=train_data, family='binomial')\n",
    "    model.fit(summarize=False)\n",
    "    \n",
    "    # Predict probabilities on the test data\n",
    "    test_data['predicted'] = model.predict(test_data)\n",
    "    \n",
    "    # Convert probabilities to binary predictions (threshold = 0.5)\n",
    "    test_data['predicted_binary'] = (test_data['predicted'] > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate log-likelihood (or another scoring metric)\n",
    "    log_likelihood = model.logLike\n",
    "    log_likelihoods.append(log_likelihood)\n",
    "    \n",
    "    # Calculate accuracy for this fold\n",
    "    accuracy = np.mean(test_data['is_correct'] == test_data['predicted_binary'])\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Summary of cross-validation results\n",
    "mean_log_likelihood = np.mean(log_likelihoods)\n",
    "mean_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "print(f\"Mean Log-Likelihood: {mean_log_likelihood}\")\n",
    "print(f\"Mean Accuracy: {mean_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703654de-e23b-42bc-ab49-b3f9cb30055b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4b443-2f2d-42d4-9dcf-73215b076e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nosise ceiling \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ae583-911d-4039-97ea-85bbc6127a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_fold_performance_evaluation():\n",
    "    regularization_param = 10**(-3)\n",
    "    max_iter=1000\n",
    "    classifier_type = 'multiclass_logistic'\n",
    "\n",
    "    model = LogisticRegression(random_state=0, C=regularization_param, max_iter=max_iter)\n",
    "    \n",
    "    X = all_df[[]]\n",
    "    values = []\n",
    "    cross_val_values = []\n",
    "\n",
    "    for prediction_variable in ['task_category', 'test_type', 'chart_type']:\n",
    "        y = all_df[prediction_variable].to_numpy()\n",
    "        scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "        for i, score in enumerate(scores):\n",
    "            row = [backbone, prediction_variable, classifier_type, regularization_param, score, i]\n",
    "            cross_val_values.append(row)\n",
    "        score_mean = np.mean(scores)\n",
    "        score_std = np.std(scores)\n",
    "        row = [backbone, prediction_variable, classifier_type, regularization_param, score_mean, score_std]\n",
    "        values.append(row)\n",
    "        # print(row)\n",
    "\n",
    "        \n",
    "    crossval_df = pd.DataFrame(\n",
    "        cross_val_values, \n",
    "        columns=['backbone', 'prediction_variable', 'classifier_type', 'regularization_param', 'crossval_acc', 'crossval_fold']\n",
    "    )\n",
    "    val_df = pd.DataFrame(\n",
    "        values, \n",
    "        columns=['backbone', 'prediction_variable', 'classifier_type', 'regularization_param', 'crossval_mean', 'crossval_std']\n",
    "    )\n",
    "    \n",
    "    return val_df, crossval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3fa957-83d0-42e6-aa42-512cb00c679b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2f2f7-15dc-482d-9322-641cf6139ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "449c5c4e-1af2-4b46-aa51-d871ede2e064",
   "metadata": {},
   "source": [
    "# Calssification of category of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d48dff-622b-4701-a054-01aec3d5f949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e4e7a-3f9d-4fd5-a00f-23ab67fdf6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_embeddings = torch.load(\"/Users/arnav/Desktop/vt-fusion/analysis/embedding_extraction/embeddings_pt/question_embeddings.pt\", weights_only=False) \n",
    "# image_embeddings = torch.load(\"/Users/arnav/Desktop/vt-fusion/analysis/embedding_extraction/embeddings_pt/image_embeddings.pt\", weights_only=False)\n",
    "# combined_embeddings = torch.load(\"/Users/arnav/Desktop/vt-fusion/analysis/embedding_extraction/embeddings_pt/combined_embeddings.pt\", weights_only=False) \n",
    "\n",
    "combined_embeddings = torch.load(\"/Users/arnav/Desktop/vt-fusion/data/embedding/multimodal_embeddings.pt\", weights_only=False)\n",
    "assert combined_embeddings.shape == (230, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586d403-0d15-4c7c-a294-7540db27f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df[prediction_variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69745477-5be4-43af-a7e0-0753d1e4a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_item_df():\n",
    "    tests = ['wainer', 'brbf', 'ggr-mc', 'vlat', 'calvi']\n",
    "    item_df = []\n",
    "    for test in tests:\n",
    "        idf = pd.read_csv(f'https://data-visualization-benchmark.s3.us-west-2.amazonaws.com/{test}/questions.csv')\n",
    "        idf['test_type'] = test\n",
    "        item_df.append(idf)\n",
    "\n",
    "    item_df = pd.concat(item_df)\n",
    "    item_df['question_image'] = item_df['question'] + \" + \" + item_df['image_file']\n",
    "    return item_df\n",
    "    \n",
    "def add_brbf_categories(r):\n",
    "    if (r['test_type'] == 'brbf'):\n",
    "        return '-'.join(r['image_file'].split(\"-\")[:-1])\n",
    "    return r['chart_type']\n",
    "\n",
    "\n",
    "item_df = create_item_df()\n",
    "item_df['chart_type_filled'] = item_df.apply(add_brbf_categories, axis=1)\n",
    "item_df['chart_type'] = item_df['chart_type_filled'].replace(chart_categories)\n",
    "# item_df = item_df[item_df['chart_type'] != 'Table']\n",
    "\n",
    "item_df['task_category'] = item_df['task_category'].apply(\n",
    "    lambda t : task_category_map[t] if t in task_category_map.keys() else t\n",
    ")\n",
    "item_df['task_category'] = item_df.apply(\n",
    "    lambda r : task_category_map[r['question']] if r['question'] in task_category_map.keys() else r['task_category'],\n",
    "    axis=1\n",
    ")\n",
    "item_df['task_category'] = item_df['task_category'].apply(lambda x : x.replace(\" \", \"-\")).to_numpy()\n",
    "item_df['chart_type'] = item_df['chart_type'].apply(lambda x : x.replace(\" \", \"-\")).to_numpy()\n",
    "\n",
    "item_df['item_id'] = item_df['question_image']\n",
    "\n",
    "print(item_df['chart_type'].unique(), item_df['task_category'].unique(), item_df['test_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06857f-c34c-4724-9c47-4dc2001dae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df_indexed = item_df.reset_index().rename(columns={'index': 'embedding_index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f559a1b-778d-4559-9eea-ad4103978084",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5e963-0ecb-433d-8a36-d33d4f74ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_response_classification_df = all_df.copy()\n",
    "item_response_classification_df = pd.merge(item_response_classification_df, item_df_indexed[['item_id', 'embedding_index']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8395c5f-8078-409b-848b-215906783f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185db6e3-83e7-46ce-9c5b-96e57e5852ba",
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30c910-3799-45e9-a198-f42e8647306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regularization_param in [10**(-3), 10**(0), 10**(3)]:\n",
    "\n",
    "def cross_fold_evaluation(embeddings, item_df, backbone = 'all-mpnet-base-v2'):\n",
    "\n",
    "    regularization_param = 10**(-3)\n",
    "    run_crossfold_val = True\n",
    "    model = LogisticRegression(random_state=0, C=regularization_param, max_iter=1000)\n",
    "    classifier_type = 'multiclass_logistic'\n",
    "    \n",
    "    X = embeddings\n",
    "    values = []\n",
    "    cross_val_values = []\n",
    "\n",
    "    for prediction_variable in ['task_category', 'test_type', 'chart_type']:\n",
    "        y = item_df[prediction_variable].to_numpy()\n",
    "        \n",
    "        if run_crossfold_val:\n",
    "            scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "            for i, score in enumerate(scores):\n",
    "                row = [backbone, prediction_variable, classifier_type, regularization_param, score, i]\n",
    "                cross_val_values.append(row)\n",
    "            score_mean = np.mean(scores)\n",
    "            score_std = np.std(scores)\n",
    "            row = [backbone, prediction_variable, classifier_type, regularization_param, score_mean, score_std]\n",
    "            values.append(row)\n",
    "            # print(row)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=743)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            row = [backbone, prediction_variable, classifier_type, regularization_param, accuracy, 1]\n",
    "            values.append(row)\n",
    "            # print(row)\n",
    "    \n",
    "    \n",
    "    crossval_df = pd.DataFrame(\n",
    "        cross_val_values, \n",
    "        columns=['backbone', 'prediction_variable', 'classifier_type', 'regularization_param', 'crossval_acc', 'crossval_fold']\n",
    "    )\n",
    "    val_df = pd.DataFrame(\n",
    "        values, \n",
    "        columns=['backbone', 'prediction_variable', 'classifier_type', 'regularization_param', 'crossval_mean', 'crossval_std']\n",
    "    )\n",
    "    \n",
    "    return val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad360ef-b7a4-44fc-846b-826c62799ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    " mm_embedding_df = cross_fold_evaluation(\n",
    "     combined_embeddings[item_response_classification_df['embedding_index'].to_numpy()], \n",
    "     item_response_classification_df, \n",
    "     backbone='meta-llama'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaac080-abc9-40e5-8577-aabf3efd054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db4d82-d590-4029-ab21-f5a7942b3eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0d089-1abb-4828-9d3e-b1ddccc11aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f3da7-dcb0-459d-a615-c6e3114b05f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367bbcb5-6aa6-4c06-8caa-a5313de91f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbones = ['all-mpnet-base-v2', 'ViT/L', 'combined']\n",
    "prediction_variables = ['test_type', 'task_category', 'chart_type']\n",
    "\n",
    "error_plot = alt.Chart(embedding_df).mark_circle().encode(\n",
    "    x=alt.X('prediction_variable:N', title=None, scale=alt.Scale(domain=prediction_variables)),\n",
    "    y=alt.Y('crossval_mean:Q', scale=alt.Scale(domain=[0,1])),\n",
    "    color='backbone:N',\n",
    "    xOffset=alt.XOffset('backbone:N', scale=alt.Scale(domain=backbones) )\n",
    ").properties(\n",
    "    width=300,\n",
    "    height=200,\n",
    ")\n",
    "\n",
    "error_bars = alt.Chart(embedding_df).mark_errorbar(extent='stdev').encode(\n",
    "    x='prediction_variable:N',\n",
    "    y='crossval_mean:Q',\n",
    "    yError='crossval_std:Q',\n",
    "    color='backbone:N',\n",
    "    xOffset=alt.XOffset('backbone:N')\n",
    ")\n",
    "\n",
    "\n",
    "# Altair chart using the custom SVG shape\n",
    "# dashed_line = alt.Chart(embedding_df).mark_point(shape=dashed_line_svg)\n",
    "\n",
    "dashed_line = alt.Chart(embedding_df).mark_text(\n",
    "    text='------------',  \n",
    "    dx=5,  \n",
    "    size=15, \n",
    "    color='black',\n",
    "    opacity=0.1\n",
    ").transform_calculate(\n",
    "    calculated_y='datum.prediction_variable == \"task_category\" ? (1/3) : datum.prediction_variable == \"test_type\" ? (1/5) : (1/14)'\n",
    ").encode(\n",
    "    x=alt.X('prediction_variable:N'),\n",
    "    y=alt.Y('calculated_y:Q')\n",
    ")\n",
    "\n",
    "\n",
    "plot = error_plot + error_bars + dashed_line\n",
    "plot.properties(width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea3632-2de5-4fcc-8166-6e3441990f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5946ab1f-f476-45e8-9275-e15907b5d676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ddcb9-ba63-452d-ac17-ec89baa86838",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i all_df\n",
    "\n",
    "# Model 1: Random intercepts model\n",
    "# model1 <- lmer(accuracy ~ 1 + (1|target_word), data=corr_df)\n",
    "\n",
    "# Model 2: Model with agentType as a fixed effect\n",
    "# model2 <- lmer(is_correct ~ agentType + (1|target_word), data=corr_df)\n",
    "\n",
    "# summary(model1)\n",
    "# summary(model2)\n",
    "\n",
    "# Logistic regression with item as a fixed effect\n",
    "\n",
    "null_model <- glmer(is_correct ~ 1 + (1 | item_id), data = all_df, family = binomial)\n",
    "test_model <- glmer(is_correct ~ test_type + (1 | item_id), data = all_df, family = binomial)\n",
    "task_model <- glmer(is_correct ~ task_category + (1 | item_id), data = all_df, family = binomial)\n",
    "graph_model <- glmer(is_correct ~ chart_type + (1 | item_id), data = all_df, family = binomial)\n",
    "graph_test_model <- glmer(is_correct ~ test_type + chart_type + (1 | item_id), data = all_df, family = binomial)\n",
    "graph_task_model <- glmer(is_correct ~ task_category + chart_type + (1 | item_id), data = all_df, family = binomial)\n",
    "task_test_model <- glmer(is_correct ~ test_type + task_category + (1 | item_id), data = all_df, family = binomial)\n",
    "graph_task_test_model <- glmer(is_correct ~ test_type + task_category + chart_type + (1 | item_id), data = all_df, family = binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f60be8-cd0e-41fe-84ac-3742fdca5cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee55b2-1f95-4040-8ab1-83f069009903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de18f0e5-458d-46a8-a98c-a33e90169c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e33e76-d6ee-4b3f-9ea0-3351d8db7b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25312bd6-d5e4-40ab-964e-b415165a24c0",
   "metadata": {},
   "source": [
    "# Participant consistency across tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed2455-0e76-407c-812e-98bc09fa8702",
   "metadata": {},
   "source": [
    "Its hard to do this as each participant only does a partial version of the test and may receive easier / harder questions\n",
    "- do we do this across paritions (e.g. people who answered the same item\n",
    "- might not be ideal ... vastly different items on tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c6947-2771-41d5-b37c-99a095a6c86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52465f-2717-4256-93db-2a2dd96f1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb690a-0a02-42f8-bb62-ec362afc01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['ggr-mc', 'vlat', 'brbf', 'wainer', 'calvi']\n",
    "data = performance_df.groupby(['participant_id', 'test_type']).mean('is_correct').reset_index()\n",
    "data = data.pivot(index='participant_id', columns='test_type', values='is_correct').reset_index().dropna()\n",
    "\n",
    "corr_df = []\n",
    "for i, t1 in enumerate(tests):\n",
    "    for j, t2 in enumerate(tests):\n",
    "        if i <= j:  # only include the bottom triangle\n",
    "            corr = np.corrcoef(data[t1], data[t2])[0][1]\n",
    "            corr_df.append({\n",
    "                'corr': corr,\n",
    "                'test1': t1,\n",
    "                'test2': t2\n",
    "            })\n",
    "\n",
    "corr_df = pd.DataFrame(corr_df)\n",
    "\n",
    "create_pairwise_agent_heatmap(corr_df, x='test1', y='test2', domain=tests, units_of_measure='corr').properties(\n",
    "    width=300,\n",
    "    height=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a60ca9-4073-4d9f-a2a7-60d31cdf0464",
   "metadata": {},
   "source": [
    "Also do a correlation across "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02aa41a-8da3-48e3-9b14-627e838e134c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0923b2-e1e3-4c10-a80f-a0700f4a771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairwise_agent_scatterplot(df, x, y, domain):\n",
    "\n",
    "    charts = []\n",
    "    for d1 in domain:\n",
    "        chart_row = []\n",
    "        for d2 in domain:\n",
    "            scatter = alt.Chart(df).mark_circle(size=50).encode(\n",
    "                y=alt.Y(d1, type='quantitative', title=d1, scale=alt.Scale(domain=[0,1])),\n",
    "                x=alt.X(d2, type='quantitative', title=d2, scale=alt.Scale(domain=[0,1])),\n",
    "                color=alt.Color('participant_id')\n",
    "            ).properties(width=50, height=50)\n",
    "            chart_row.append(scatter)\n",
    "        charts.append(chart_row)\n",
    "\n",
    "    return alt.vconcat(\n",
    "        *[alt.hconcat(*chart_row) for chart_row in charts]\n",
    "    )\n",
    "\n",
    "\n",
    "create_pairwise_agent_scatterplot(data, x='test1', y='test2', domain=tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1b769-be4e-4ebf-9ef3-59c0d5ed4d22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6400ab78-1bf2-4316-b56c-0d4f36f2a432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
